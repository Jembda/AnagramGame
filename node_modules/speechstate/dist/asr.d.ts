/// <reference types="webspeechapi" />
import { RecogniseParameters, Hypothesis, AzureCredentials } from "./types";
interface MySpeechRecognition extends SpeechRecognition {
    new (): any;
}
interface MySpeechGrammarList extends SpeechGrammarList {
    new (): any;
}
type ASREvent = {
    type: "READY";
    value: {
        wsaASR: MySpeechRecognition;
        wsaGrammarList: MySpeechGrammarList;
    };
} | {
    type: "ERROR";
} | {
    type: "NOINPUT";
} | {
    type: "CONTROL";
} | {
    type: "START";
    value?: RecogniseParameters;
} | {
    type: "STARTED";
    value: {
        wsaASRinstance: MySpeechRecognition;
    };
} | {
    type: "STARTSPEECH";
} | {
    type: "RECOGNISED";
} | {
    type: "RESULT";
    value: Hypothesis[];
};
interface ASRContext extends ASRInit {
    azureAuthorizationToken?: string;
    wsaASR?: MySpeechRecognition;
    wsaASRinstance?: MySpeechRecognition;
    wsaGrammarList?: MySpeechGrammarList;
    result?: Hypothesis[];
    params?: RecogniseParameters;
}
interface ASRInit {
    asrDefaultCompleteTimeout: number;
    asrDefaultNoInputTimeout: number;
    locale: string;
    audioContext: AudioContext;
    azureCredentials: string | AzureCredentials;
}
export declare const asrMachine: import("xstate").StateMachine<ASRContext, ASREvent, Record<string, import("xstate").AnyActorRef>, import("xstate").ProvidedActor, import("xstate").ParameterizedObject, import("xstate").ParameterizedObject, string, import("xstate").StateValue, string, ASRInit, {}, import("xstate").ResolveTypegenMeta<import("xstate").TypegenDisabled, ASREvent, import("xstate").ProvidedActor, import("xstate").ParameterizedObject, import("xstate").ParameterizedObject, string, string>>;
export {};
//# sourceMappingURL=asr.d.ts.map